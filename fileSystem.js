
// --- Generated by Python Script ---
const fileSystem = {
    'type': 'dir',
    'children': {
    "cv.md": {
        "type": "file",
        "content": ".5\n\n**[Emily M. Boudreaux (they/she)](https://www.emilyboudreaux.com)**\n\n.5\n\nDartmouth College, Hanover NH, 03784\n\n.5\n\nDepartment of Physics and Astronomy\n\n.5\n\nHB 6127\n\n.5\n\nemily\\@boudreauxmail.com\n\nRESEARCH INTERESTS\n==================\n\nComputational astrophysics, stellar evolution, low mass stars.\n\nJOBS\n====\n\n-   **Dartmouth College**, May 2024 - present\\\n    Post Doctoral Research Associate working with [Aaron\n    Dotter](https://aarondotter.github.io/) on\n    [4D-STAR](https://fys.kuleuven.be/ster/research-projects/4d-star)\n\n-   **Dartmouth College**, May 2024 - present\\\n    Lecturer\n\nEDUCATION\n=========\n\nDartmouth College Hanover, NH\\\n\u00a0\u00a0*Master of Science* --- Physics & Astronomy May 2022\\\n\u00a0\u00a0\u00a0\u00a0Advisor: [Dr. Brian C.\nChaboyer](https://physics.dartmouth.edu/people/brian-charles-chaboyer)\\\n\u00a0\u00a0\u00a0\u00a0Secondary Advisor: [Dr. Elisabeth R.\nNewton](https://physics.dartmouth.edu/people/elisabeth-newton)\\\n\u00a0\u00a0*Doctor of Philosophy* ---- Physics & Astronomy April 2024\\\n\u00a0\u00a0\u00a0\u00a0Thesis: [Models of Low Mass Stars as Physical\nLaboratories](https://raw.githubusercontent.com/tboudreaux/thesis/master/thesis/Thesis.pdf)\\\n\u00a0\u00a0\u00a0\u00a0Advisor: [Dr. Brian C.\nChaboyer](https://physics.dartmouth.edu/people/brian-charles-chaboyer)\\\n\u00a0\u00a0\u00a0\u00a0Commitee: Dr. Aaron Dotter, Dr. Elisabeth R. Newton, Dr. Jamie\nTayar\\\nHigh Point University High Point, NC\\\n\u00a0\u00a0*Bachelor of Science*, **summa cum laude** --- Computational Physics\nMay 2019\\\n\u00a0\u00a0\u00a0\u00a0Advisor: [Brad N. Barlow](https://tarheels.live/bbarlow/)\\\n\nPUBLICATIONS\n============\n\nFirst Author\n------------\n\n-   **Boudreaux, E.M.**, Garcia Soto, Aylin., Chaboyer, B.C., 2024.\n    [Correlations between Ca II H&K Emission and the Gaia M dwarf\n    Gap](https://ui.adsabs.harvard.edu/abs/2024arXiv240214984B/abstract),\n    *The Astrophysical Journal*. 965(1), p.56\n\n-   **Boudreaux, E.M.**, Chaboyer, B.C., Ash, Amanda., Edaes Hoh,\n    Renata., Feiden, Gregory., 2024. [Chemically Self-Consitent Modeling\n    of the Globular Cluster NGC 2808 and its Effects on the Inferred\n    Helium abundance of Multiple Stellar\n    Populations](https://ui.adsabs.harvard.edu/abs/2024arXiv241117562B/abstract),\n    *The Astrophysical Journal*. 980(1), p.180\n\n-   **Boudreaux, E.M.**, Chaboyer, B.C., 2023. [Updated High-Temperature\n    Opacties for the Dartmouth Stellar Evolution Program and their\n    Effect on the Jao Gap\n    Location](https://ui.adsabs.harvard.edu/abs/2023arXiv230110798B/abstract),\n    *The Astrophysical Journal*. 944(2), p.129\n\n-   **Boudreaux, E.M.**, Newton, E.R., Mondrik, N., Charbonneau, D.,\n    Irwin, J., 2021. [The Ca II H&K Rotation-Activity Relation in 53\n    mid-to-late type\n    M-Dwarfs](https://ui.adsabs.harvard.edu/abs/2022ApJ...929...80B/abstract),\n    *The Astrophysical Journal*. 926(1), p.80\n\n-   **Boudreaux, T. M.**, 2017, \"[The applications of deep neural\n    networks to sdBV\n    classification](https://ui.adsabs.harvard.edu/#abs/2017OAst...26..258B/abstract)\",\n    *Open Astronomy*, **26**, 258.\n\n-   **Boudreaux, E. M.**, Barlow, B. N., Fleming, S. W., Soto, A. V.,\n    Million, C., Reichart, D. E., Haislip, J. B., Linder, T. R.,\n    Moore, J. P., 2017. \"[A search for rapidly pulsating hot subdwarf\n    stars in the GALEX\n    survey](https://ui.adsabs.harvard.edu/#abs/2017ApJ...845..171B/abstract)\",\n    *Astrophysical Journal*, **845**, 171.\n\nContributing Author\n-------------------\n\n-   Ying, M., Chaboyer, B., **Boudreaux, E.M.**, Slaughter, C.,\n    Boylan-Kolchin, M., Wesiz, D., [The Absolute Age of\n    M92.](https://ui.adsabs.harvard.edu/abs/2023AJ....166...18Y/abstract)\n    *The Astronomical Journal*, 166(1), p.18.\n\n-   Guidry, J.A., Vanderbosch, Z.P., Hermes, J.J., Barlow, B.N., Lopez,\n    I.D., **Boudreaux, E.M.**, Corcoran, K.A., Bell, K.J., Montgomery,\n    M.H., Heintz, T.M. and Castanheira, B.G., 2021. [I Spy Transits and\n    Pulsations: Empirical Variability in White Dwarfs Using Gaia and the\n    Zwicky Transient\n    Facility.](https://ui.adsabs.harvard.edu/abs/2021ApJ...912..125G/abstract)\n    *The Astrophysical Journal*, 912(2), p.125.\n\n-   Vos, J., Vuc\u0306kovic\u0300, M., Chen, X., Han, Z., **Boudreaux, E. M.**,\n    Barlow, B. N., \u00d8stensen, R., N\u00e8meth, P., 2019, \"[The orbital period\n    --- mass ratio relation of wide sdB+MS binaries and its application\n    to the stability of\n    RLOF.](https://ui.adsabs.harvard.edu/#abs/2019MNRAS.482.4592V/abstract)\\\",\n    *Monthly Notices of The Royal Astronomical Society*, **482**, 4592\n\nCOMPUTING SKILLS\n================\n\n-   *Programming Languages:*\n\n    1.  *Expert:* Python, C, C++(11/17/20/23), Fortran (77/90)\n\n    2.  *Comfortable:* Arduino, PHP, JavaScript, Mathematica\n\n    3.  *Familiar:* Go, Rust\n\n-   *Numerical Tools:* Finite Element Modeling (MFEM, deal.II)\n\n-   *Web Backend Technologies:* Flask, MongoDB, MySQL, MariaDB\n\n-   *Misc:* Period04, Docker, GitHub, ZFS, LaTeX, Bash, Zsh\n\nINTERNSHIPS\n===========\n\n-   **Harvard Smithsonian Astrophysical Observatory**, 2018\\\n    Harvard SAO REU Student\n\n-   **Space Telescope Science Institute**, 2016\\\n    SASP Summer Intern\n\nAWARDS & HONORS\n===============\n\n-   **Dartmouth College Department of Physics & Astronomy** 2024\\\n    Selamawit Tsehaye Teaching Award\n\n-   **Dartmouth College Department of Physics & Astronomy** 2019\\\n    Department Chair Fellowship\n\n-   **The National Science Foundation**, 2019\\\n    Graduate Record Fellowship Program Honorable Mention\n\n-   **High Point University**, 2019\\\n    University Award for Highest Achievement\n\n-   **High Point University Honors Scholar Program**, 2019\\\n    All University Honors\n\n-   **The Barry Goldwater Scholarship and Excellence in Education\n    Foundation**, 2018\\\n    Goldwater Scholar in Mathematics, Science, and Engineering\n\n-   **High Point University Department of Physics**, 2018\\\n    Endowed Scholarship\n\n-   **National Collegiate Honors Council**, 2018\\\n    Portz Scholarship\n\n-   **Sigma Xi, The Scientific Research Honors Society**, 2018\\\n    Elected Associate Member\n\n-   **Sigma Pi Sigma, National Physics Honor Society**, 2018\\\n    Elected Member\n\n-   **The Barry Goldwater Scholarship and Excellence in Education\n    Foundation**, 2017\\\n    Honorable Mention for excellence in Mathematics, Science, and\n    Engineering\n\n-   **High Point University**, 2015--2019\\\n    Presidential Scholarship\n\nSELECTED ORAL PRESENTATIONS\n===========================\n\n-   **Sixth Challenges and Innovations in Computational Astrophysics**,\n    2025 (upcoming), ISER Mohali,\u00a0India\\\n    \"New Dimensions in Stellar Structure and Evolution.\" \\[Invited\\]\n\n-   **Twelfth Annual Meeting on Hot Subdwarfs and Related Objects**,\n    2025 (upcoming), Little Switzerland NC, USA\\\n    \"New Dimensions in Stellar Structure and Evolution.\"\n\n-   **National Collegiate Honors Council Annual Meeting**, 2018, Boston,\n    MA\\\n    \"The Applications of Deep Neural Networks to sdBV Classification\"\n    \\[Invited\\]\n\n-   **North Carolina Astronomers Meeting**, 2017, Greensboro, NC\\\n    \"The Applications of Deep Neural Networks to sdBV Classification\"\n\n-   **Eighth Annual Meeting on Hot Subdwarfs and Related Objects**,\n    2017, Krak\u00f2w, Poland\\\n    \"The Applications of Deep Neural Networks to sdBV Classification\"\n\n-   **High Point University Research and Creative Works Symposium**,\n    2017, High Point, NC\\\n    \"A Virtual Survey of all known Hot Subdwarfs -- searching for p-mode\n    pulsations with GALEX\"\n\n-   **Meeting of Astronomers in South Carolina**, 2017, Greenville, SC\\\n    \"The Applications of Deep Neural Networks to Time Domain\n    Astrophysics\"\n\nSELECTED POSTER PRESENTATIONS\n=============================\n\n-   **21st Meeting on Cool Stars**, 2022, Toulouse France\\\n    Updated High-Temperature Opacties for DSEP and Their Effect on the\n    Jao Gap Location\n\n-   **233rd Meeting of the American Astronomical Society**, 2019,\n    Seattle Washington\\\n    \"[A Journey to Mars: HPUniverse Day and Its Impact on Young Minds\n    and a\n    Community.](https://ui.adsabs.harvard.edu/#abs/2019AAS...23314705C/abstract)\"\n\n-   **233rd Meeting of the American Astronomical Society**, 2019,\n    Seattle Washington\\\n    \"[Effects of the Primordial Binary Fraction on the Evolution of\n    Globular\n    Clusters.](https://ui.adsabs.harvard.edu/#abs/2019AAS...23324909B/abstract)\"\n\n-   **231st Meeting of the American Astronomical Society**, 2018,\n    Washington D.C.\\\n    \"[Using Deep Learning to Analyze the Voices of\n    Stars.](https://ui.adsabs.harvard.edu/#abs/2018AAS...23115029B/abstract)\"\n\n-   **227th Meeting of the American Astronomical Society**, 2016,\n    Kissimmee, FL\\\n    \"[New Long Period Hot Subdwarfs from the Hobby-Eberly\n    Telescope](https://ui.adsabs.harvard.edu/#abs/2016AAS...22734412B/abstract)\"\n\nTEACHING EXPERIENCE Instructor of Record\n========================================\n\n-   **Dartmouth College**, Fall 2024\\\n    Astrophysics (*Astr(1)74*)\n\nTEACHING EXPERIENCE Teaching Staff\n==================================\n\n-   **High Point University**, 2016,2017\\\n    Multivariable Calculus (*MTH 2410*, SI)\n\n-   **Dartmouth College**, 2022\\\n    Advanced Stellar Astrophysics (*Astr 115*, TA)\n\n-   **Dartmouth College**, 2021,2022\\\n    Public Obsserving (TA)\n\n-   **Dartmouth College**, 2020\\\n    Introductory Mechanics (*Phys 13*, TA)\n\n-   **Dartmouth College**, 2020, 2023\\\n    Introductory Solar System Astronomy (*Astr 1*, TA, 7 Lectures)\n\n-   **Dartmouth College**, 2023\\\n    Stars and the Milky Way (*Astr 15*, TA)\n\n-   **Dartmouth College**, 2024\\\n    The Development of Astronomical Thought (*Astr 4*, TA)\n\nREFEREE SERVICE\n===============\n\n-   **The Astrophysical Journal, IOP**, 2025\n\n-   **Nature Physics, Nature Portfolio**, 2024\n\nMENTORSHIP & STUDENTS\n=====================\n\n-   **Renata Edaes Hoh**, **Dartmouth College, WISP**, 2022\\\n    Identifying zero point offsets between absolute and differential\n    photometry the globular cluster NGC 2808.\n\n-   **Mayumi Liz de Andrade Miyazato**, **Dartmouth College, WISP**,\n    2023\\\n    Identifying zero point offsets between absolute and differential\n    photometry the globular clusters NGC 6752 & 47 Tuc.\n\nSELECTED SOFTWARE\n=================\n\nAll of my software can be found on my\n[GitHub](https://github.com/tboudreaux) page.\n\n-   [OPAT](https://github.com/4D-STAR/opat-core) --- C++ and Python\n    interface for OPAT file format.\n\n-   [libmesac](https://github.com/4D-STAR/libmesac) --- C interface for\n    much of the MESA microphysics and numerical libraries\n\n-   [CoolDwarf](https://github.com/tboudreaux/CoolDwarf) --- Three\n    dimensional brown dwarf structure cooling model.\n\n-   [fidanka](https://github.com/tboudreaux/fidanka) --- Robust CMD\n    fiducial line extractor and isochrone fitter.\n\n-   [pyTOPSScrape](https://github.com/tboudreaux/pytopsscrape) ---\n    Custom python API for the Los Alamos OPLIB High-Temperature Opacties\n    tables.\n\n-   [mplEasyAnimate](https://github.com/tboudreaux/mpl_animate) ---\n    Simple and easy animation library for use with matplotlib.\n\n-   [pubPolishPy](https://github.com/tboudreaux/PubPolishPy) ---\n    Automatically rebuilt LaTeX project to target different journals.\n\n-   [splitAxes](https://github.com/tboudreaux/splitAxes) --- An easy way\n    to build complex split axes graphs in matplotlib.\n\n-   [PolytropicStellarModel](https://github.com/tboudreaux/PolytropicStellarModel)\n    --- A blazingly fast, GPU accelerated, polytrope solver.\n\nVOLUNTEER WORK\n==============\n\n-   **Dartmouth College**, 2024\\\n    Dartmouth Astronomy Night\n\n-   **Dartmouth College**, 2023--2024\\\n    Dartmouth Physics and Astronomy Graduate Curriculum Committee\n\n-   **Dartmouth College**, 2020,2021,2022,2023\\\n    Public Observing\n\n-   **The Hopkins Center for the Arts**, 2022\\\n    Pre-Movie Public Science Talk\n\n-   **Montshire Museum of Science**, 2020,2022,2023\\\n    Astronomy Day -- Comet Making, Ask an Astronomer, Star Clock\n\n-   **High Point University**, 2015,2016,2017,2018,2019\\\n    HPUniverse Day -- Finding Exoplanets\n\nRESEARCH PROJECTS\n=================\n\n-   Development of a next generation 3+1D Stellar Structure and\n    Evolution Program, 2024--\n\n-   The effects of OPLIB opacities and mutliple populations on the\n    location of the Red Giant Branch Bump, 2024--2025\n\n-   The Jao Gap width and location as a population age indicator,\n    2022--2024\n\n-   The effect of Opacties on the location of the Jap Gap, 2021--2023\n\n-   Modifying the Dartmouth Stellar Evolution Program to fully self\n    consistantly handel increased He abundance, 2020--2024\n\n-   The Ca II H&K Rotation-Activity Relation in 50 early-to-late type\n    M-dwarfs, 2019--2020.\n\n-   Effects of the Primordial Binary Fraction on Globular Cluster\n    Evolution, 2018\n\n-   Applications of Deep Learning to Classification of PTF Data, 2018\n\n-   Applications of Machine Learning to the Classification of Pulsating\n    Stars, 2017--2018\n\n-   A Search for Rapidly Pulsating Hot Subdwarfs in the GALEX Survey,\n    2016--2017\n\n-   Orbital Solution Analysis of Long Period sdB+F/G/K Binaries,\n    2015--2016\n"
    },
    "papers.md": {
        "type": "file",
        "content": "### First Author\n\n- Boudreaux, E.M., Garcia Soto, Aylin., Chaboyer, B.C., 2024. Correlations between Ca II H&K Emission and the Gaia M dwarf Gap, The Astrophysical Journal. 965(1), p.56\n\n- Boudreaux, E.M., Chaboyer, B.C., Ash, Amanda., Edaes Hoh, Renata., Feiden, Gregory., 2024. Chemically Self-Consitent Modeling of the Globular Cluster NGC 2808 and its Effects on the Inferred Helium abundance of Multiple Stellar Populations, The Astrophysical Journal. 980(1), p.180\n\n- Boudreaux, E.M., Chaboyer, B.C., 2023. Updated High-Temperature Opacities for the Dartmouth Stellar Evolution Program and their Effect on the Jao Gap Location, The Astrophysical Journal. 944(2), p.129\n\n- Boudreaux, E.M., Newton, E.R., Mondrik, N., Charbonneau, D., Irwin, J., 2021. The Ca II H&K Rotation-Activity Relation in 53 mid-to-late type M-Dwarfs, The Astrophysical Journal. 926(1), p.80\n\n- Boudreaux, T. M., 2017, \"The applications of deep neural networks to sdBV classification\", Open Astronomy, 26, 258.\n\n- Boudreaux, E. M., Barlow, B. N., Fleming, S. W., Soto, A. V., Million, C., Reichart, D. E., Haislip, J. B., Linder, T. R., Moore, J. P., 2017. \"A search for rapidly pulsating hot subdwarf stars in the GALEX survey\", Astrophysical Journal, 845, 171.\n\n### Contributing Author\n\n- Ying, M., Chaboyer, B., Boudreaux, E.M., Slaughter, C., Boylan-Kolchin, M., Wesiz, D., The Absolute Age of M92. The Astronomical Journal, 166(1), p.18.\n\n- Guidry, J.A., Vanderbosch, Z.P., Hermes, J.J., Barlow, B.N., Lopez, I.D., Boudreaux, E.M., et al., 2021. I Spy Transits and Pulsations: Empirical Variability in White Dwarfs Using Gaia and the Zwicky Transient Facility. The Astrophysical Journal, 912(2), p.125.\n\n- Vos, J., Vu\u010dkovi\u0107, M., Chen, X., Han, Z., Boudreaux, E. M., et al., 2019, \"The orbital period mass ratio relation of wide sdB+MS binaries and its application to the stability of RLOF.\", Monthly Notices of The Royal Astronomical Society, 482, 4592\n\n"
    },
    "projects": {
        "type": "dir",
        "children": {
            "CoolDwarf": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "![Cool Dwarf Logo](assets/logo/logo-medium.png)\n# CoolDwarf\n\nCoolDwarf is a dynamic brown dwarf cooling model meant to cover short time scales with high accuracy. That is to say \nthat CoolDwarf is not meant to trace cooling over gigayear, megayear, or even kiloyear timescales; rather, CoolDwarf is meant\nto trace cooling over minutes, hours, days, and year timescales. \n\nThe original reason for this project was to model the surface temperature profile of a brown dwarf + Mdwarf binary where\nthe M dwarf exhibits a high degree of XRay flux (from flares).\n\n\n## CoolDwarf is still *very* early in its development and should not be taken as a scientificly useful software yet!\n\nDevelopment is ongoing and I hope to have a realease within the next 6 months. \n\n## Installation\nIf you with to install CoolDwarf for testing or development I reccomend using virtual enviroment and installing\nusing pip\n\n```bash\ngit clone https://github.com/tboudreaux/CoolDwarf\ncd CoolDwarf\npip install . -e \n```\n\n## Documentation\nThe documentation for CoolDwarf may be found <a href=\"https://tboudreaux.github.io/CoolDwarf/\">here</a>\n\n\n"
                    }
                }
            },
            "Fidanka": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "<p align=\"center\">\n\t<img width=\"460\" height=\"460\" src=\"/assets/fidankaLogo.png\">\n</p>\n\n---\n\n<div align=\"center\">\n\n[![Python - Linux](https://github.com/tboudreaux/fidanka/actions/workflows/python-build.yml/badge.svg)](https://github.com/tboudreaux/fidanka/actions/workflows/python-build.yml)\n[![python](https://img.shields.io/badge/Python-3.9-3776AB.svg?style=flat&logo=python&logoColor=white?style=for-the-badge)](https://www.python.org)\n[![codecov](https://codecov.io/gh/tboudreaux/fidanka/branch/master/graph/badge.svg?token=DQ5CUJ7WNA)](https://codecov.io/gh/tboudreaux/fidanka)\n\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n[![Forks](https://img.shields.io/github/forks/tboudreaux/fidanka.svg?style=for-the-badge)](https://github.com/tboudreaux/fidanka)\n[![Stars](https://img.shields.io/github/stars/tboudreaux/fidanka.svg?style=for-the-badge)](https://github.com/tboudreaux/fidanka)\n\n</div>\n\n---\n\n# Fidanka\nFidanka is an (hopefully) easy to use python package for doing science with globular clusters.\nWritten by Emily M. Boudreaux and Martin Ying.\n\n\n## Docs & Information\nThe full API Documentation can be found <a href=\"https://tboudreaux.github.io/fidanka/\">here</a>.\n\nA short presentation which I've given on fidanka may be found <a href=\"https://algebrist.ddns.net/~tboudreaux/presentations/VAST/#/\">here</a>\n\n## Install\n\n### PyPi\nFidnka will be avalible on PyPi soon; however, it is not currently avalible anywhere other than GitHub.\n\n### From Source (latest)\n\n```bash\ngit clone https://github.com/tboudreaux/fidanka.git\ncd fidanka\npip install .\n```\n\n### Development\nIn order to develop for fidanka it will be helpful to also install a few more packages. We also recommend you work in a virtual environment.\nMoreover, you must work in your own fork of fidanka and issue pull requests. First fork fidanka on GitHub.\n```bash\nconda create --name fidankaDev python=3\npip install pre-commit\npip install commitizen\ngit clone https://github.com/<your-username>/fidanka.git\npip install -e .\npre-commit install\npre-commit autoupdate\n```\nWhile use commitizen is not required when contributing, it is highly encouraged.\nIf you do use commitizen then the add commit workflow is as follows\n\n```bash\ngit add <files>\ncz c\n```\n\n## Contributing\nWe welcome any contributions made to this project. Please work in a fork and submit a pull request. We also request that you use the pre-commit hooks we have defined and commit using commitizen. This will help us keep a cleaner channelog. Finally, any contributors to this project are expected to behave in accordance with the code of conduct (code_of_conduct.md). Any violation of this will result in a ban on contributions and an evaluation of whether previous contributions should be purged.\n\n## Examples\n\n### Measuring the fiducial lines of a cluster with multiple populations\nAssuming you have your photometry stored in some data structure (here I retrieve it from a pickle\nas a pandas dataframe), and you have a prior that there are 2 populations within the\ncluster, you can measure those fiducial lines as follows\n\nThis measurement will re-sample the data 1000 times and remeasure the fiducial lines each time\nin order to get confidence intervals and a mean.\n```python\nfrom fidanka.fiducial import measure_fiducial_lines\nimport pickle as pkl\n\nPHOTROOT = \"photometry.pkl\"\nwith open(PHOTROOT, 'rb') as f:\n    photometry = pkl.load(f)[1]\n\nMC = 1000\nfiducialLine = measure_fiducial_lines(\n        photometry[\"F275W\"],\n        photometry[\"F814W\"],\n        photometry[\"F275W_RMS\"],\n        photometry[\"F814W_RMS\"],\n        reverseFilterOrder=True,\n        mcruns=MC,\n        nPops = 2\n        )\npopA = fiducialLine[0]\npopB = fiducialLine[1]\n\npopAMean = popA.mean\npopA5th = popA.confidence(0.05)\npopA95th = popA.confidence(0.95)\n\nwith open('fldump-sorted.pkl', 'wb') as f:\n    pkl.dump(fiducialLine, f)\n```\n\n### Fitting an Isochrone\n\nImagine a file called HUGS1.csv which contains photometry and can be read with\npandas for the globular cluster NGC 2808 in your current working directory.\nAlso imagine an isochrone called iso.txt (In the MIST format) in the current working directory. We\ncan fit that isochrone to the photometry as follows. Also imagine we still\nhave popA loaded from the previous example. Finally, imagine you have a series of bolometric\ncorrection tables in the current directory stored in a folder called \"bolTables\".\nThese tables should be in the format available on the MIST website.\n\n```python\nfrom fidanka.isochrone.MIST import read_iso\nfrom fidanka.isofit.fit import optimize\n\nimport pandas as pd\nimport re\nimport os\n\nbolFilenames = list(filter(lambda x: re.search(\"feh[mp]\\d+\", x), os.listdir(\"bolTables\")))\nbolPaths = list(map(lambda x: os.path.join(args.bTabs, x), bolFilenames))\nFeH = ...\n\nphotometry = pd.read_csv(\"HUGS1.csv\")\niso = read_iso(\"ISO/mist.iso\")\n\nfilter1 = photometry[\"F275W\"]\nfilter2 = photometry[\"F814W\"]\nerror1 = photometry[\"F275W_RMS\"]\nerror2 = photometry[\"F814W_RMS\"]\n\nbestFitResults = optimize(\n    popA.mean,\n    iso,\n    bolPaths,\n    FeH,\n    filters = (\"F275W\", \"F814W\", \"F814W\")\n)\n\nprint(bestFitResults)\n```\n\n### logging\nfidanka will write some information to stdout; however, more extensive information will be writte\nto a log file. By default this is called fidanka.log; however, its name and log level can be\nconfigured with the get_logger function\n\n```python\nfrom fidanka.misc.utils import get_logger\nimport logging\n\nget_logger(\"rootLoggerName\", \"testRun.log\", clevel=logging.INFO)\n```\n\nThis will result in much more information being written to std out. The first\nargument is the name of the logger module and can be whatever you like. The second\nis the filename for the file handler. There are also keyword arguments\nclevel, flevel, and level which control the minimum logger level to be written\nto the console, the file, and either respectively.\n\n\n### Population Synthethis\nAssume you have a series of isochrones loaded in a list and you want to generate\na 12.5Gyr cluster with 30000 members and a binary fraction of 0.25.\n\n```python\nfrom fidanka.population.synthesize import population\nfrom fidanka.fiducial.fiducial import measure_fiducial_line\n\n\nARTSTARTEST = \"ArtificialStarCalibratedErrorFunctions.pkl\"\nwith open(\"./RMSFuncs.pkl\", 'rb') as f:\n    rmsFuncs = pkl.load(f)\nwith open(ARTSTARTEST, 'rb') as f:\n    artStar = pkl.load(f)\n\n\ntargetAge = 12.5e9\nn = 30000\n\npop = population(\n    isos[:2],\n    -0.84,\n    0.25,\n    lambda x: x-x + targetAge,\n    n,\n    targetAge,\n    targetAge,\n    0.25,\n    2,\n    artStar,\n    9198,\n    0.17,\n    \"F606W\"\n)\n\n# Note that the population Synthesis runs when the code\n# gets here, NOT at population instantiation time\n# this can also be called with pop.data().\n# If this function is called multiple times the same results\n# will be returned for the same object as a cache is used\ndf = pop.to_pandas()\npop.to_csv(\"TestPop.csv\")\n```\n\n### Funding and Acknowledgements\nWe recongnize the support of NASA Grant No. 80NSSC18K0634 in the development of this software.\n\n"
                    }
                }
            },
            "GridFire": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "<p align=\"center\">\n  <img src=\"assets/logo/GridFire.png\" width=\"300\" alt=\"OPAT Core Libraries Logo\">\n</p>\n\n\n---\n\nGridFire is a C++ library designed to preform general nuclear network evolution using the reaclib library. It is a\npart of the larger SERiF project which is itself a part of the 4D-STAR collaboration. \n\n> Note that GridFire is still very early in development and is not ready for scientific use. The API is not stable and\n> the reults are not yet validated. We are actively working on improving the library and once it is tested and validated\n> we will release a stable version.\n\n## Design\n\nGridFire uses a \"graph-first\" design to represent the nuclear network. Specifically, internally the network is\nrepresented as a directed hypergraph where nodes are nuclides and edges are reactions. This design allows for very straightforward\nand efficient evolution of the network topology. \n\n## Current Features\n- **Reaclib Support**: GridFire uses reaclib coefficients statically compiled into the binary to evaluate reation rates. We bundle a script which can be used to generate the header files where these are stored. \n- **Auto Differentiation**: GridFire uses [CppAD](https://github.com/coin-or/CppAD) to generate analytic Jacobians for stiff network evaluation.\n- **Dynamic Network Topology**: GridFire supports dynamic network topology, allowing for the addition and removal of nuclides and reactions during runtime.\n- **Dynamic Stiff Detection**: GridFire uses a heuristic stiff detection algorithm to determine the stiffness of the network at runtime.\n\n## Planned Features\n### High Priority\n- **Reverse Reactions**: Currently, GridFire only supports forward reactions. Very soon we will add support for reverse reactions.\n- **Weak Reactions**: GridFire will eventually support weak reactions, allowing for a more complete nuclear network evolution.\n- **Python Bindings**: We plan to add Python bindings to GridFire, allowing for easy integration with Python-based workflows.\n\n### Low Priority\n- **GPU and Multi-Core Support**: We plan to add support for GPU and multi-core execution, allowing for faster network evolution.\n- **Topology Optimization**: We plan to add support for topology optimization, allowing for more efficient network evolution.\n- **Real-Time Network Visualization**: We plan to add support for real-time network visualization, allowing for better understanding of the network evolution.\n\n## Building\nGridFire uses meson as its build system. The minimum required version is 1.5.0. To build GridFire, you will need to have the following dependencies installed:\n\n- C++ compiler supporting at least C++20 (though we test against C++23, and it is **strongly** recommended to use C++23)\n- [Meson](https://mesonbuild.com/) build system (`apt install meson` or `brew install meson` or `pip install \"meson>=1.5.0\"`)\n- [Ninja](https://ninja-build.org/) build system (`apt install ninja-build` or `brew install ninja` or `pip install ninja`)\n- (_optional to have preinstalled_) [Boost](https://www.boost.org/) libraries (`apt install libboost-all-dev` or `brew install boost`)\n\nBoost is labeled as optional because if GridFire cannot find boost during installation it will ask the user if they want\nto install it, and then it will take care of that insallation. \n\nOnce at least the C++ compiler, meson, and ninja are installed, you can build GridFire by running the following commands in the root directory of the repository:\n\n```bash\nmeson setup build --buildtype=release\nmeson compile -C build\nmeson install -C build\n```\n\nRunning the first command will create a `build` directory in the root of the repository, which will contain all the\nbuild files. The second command will compile the library and the third command will install it to your system.\nWhen installing GridFire it will generate a `pkg-config` file which can be used to link against the library in other\nprojects.\n\n### Linking Against GridFire\nThe easiest way, by far, to link against GridFire is to use `pkg-config`. This will automatically handle all the\nlibrary and include flags for you. Further, the `pkg-config` file will also include the required information on\nthe libcomposition library so that you do not need to also manually link against it.\n\n```bash\ng++ -o main.cpp $(pkg-config --cflags --libs gridfire)\n```\n\n### Meson Wrap Installed Dependencies\nAside from Boost, GridFire dependes on a few other libraries which are not required to be installed on the system, but\nare instead downloaded and built as part of the GridFire build process. These are:\n- [CppAD](https://github.com/coin-or/CppAD): Used for automatic differentiation.\n- [libcomposition](https://github.com/4d-star/libcomposition): Used for composition of networks.\n- [libconfig](https://github.com/4d-star/libconfig): Used for configuration management.\n- [libconstants](https://github.com/4d-star/libconstants): Used for physical constants.\n- [liblogging](https://github.com/4d-star/liblogging): Used for logging.\n\nThese will all be downloaded and build automatically by meson when you run the `meson setup` and `meson compile` commands.\n\n## Usage\n\n> All code in GridFire is within the `gridfire` namespace. This means that you will need to use `gridfire::` prefix when\n> using any of the classes or functions in GridFire.\n\n> All code in lib* libraries is within the `fourdst::` namespace or subnamespaces of this (`fourdst::composition`,\n`fourdst::config`, etc...). This means that you will need to use `fourdst::` prefix when using any of the classes or\n> functions in these libraries.\n\nGridFire is designed to primarily interface with `fourdst::composition::Composition` objects (provided by libcomposition).\nThe idea is that you create a composition object which tracks some set of species and either their mass fractions or number\nfractions. You then initialize a network from this composition object. \n\nWhen evaluating the network you pass the current state of the composition object, the temperature, the density, and the \ncurrent energy of the system (along with some other parmeters such as the maximum time and the initial time step). Any\nnetwork implemented in GridFire will then use this information to evolve the network and return a `gridfire::NetOut` object\nwhich contains the updated state of the composition object, the new energy of the system, and the number of steps the\nsolver took to reach the final state.\n\nThere are a few networks currently implemented in GridFire.\n\n| Network Name               | Description                                                                                                                                                                             | Network Format Enum |\n|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|\n| `gridfire::GraphNetwork`   | A general nuclear network which uses a directed hypergraph to represent the network topology. This is the main network implemented in GridFire and is the one you will most likely use. | `REACLIB`         |\n| `gridfire::StaticGraphNetwork` |  A less general version of `gridfire::GraphNetwork` which uses a static graph to represent the network topology. That is to say that new species cannot be added or removed after instatiation of the network. | `REACLIB_STATIC`  |\n| `gridfire::Approx8Network` | A specialized network for the approximate 8 species network used in stellar nucleosynthesis. This is a reimplimentation of the network developed by Frank Timmes' for MESA              | `APPROX8`          |\n| `gridfire::Network`        | A virtual base class for all networks in GridFire. This is not meant to be used directly; rather, all networks in GridFire should subclass `gridfire::Network` and implement the required `evaluate` method. | `DEFAULT`         |\n\nRegardless of the network you chose, the interface is the same. You will need to create a `gridfire::NetIn` object\nwhich contains the initial state of the network, including the composition, temperature, density, energy, initial time\nstep, and maximum time. You will then pass this object to the `evaluate` method of the network you chose. The `evaluate`\nmethod will return a `gridfire::NetOut` object which contains the updated state of the network.\n\nA simple example of how to use `gridfire::GraphNetwork` is shown below (note that `composition.h` is a header file\nprovided by libcomposition, which means your compiler must be able to find its header and library files. The pkg-config\nfile generated during the installation of GridFire will take care of this for you, so you can use `pkg-config` to\ncompile this example):\n\n```c++\n#include <gridfire/netgraph.h>\n#include <fourdst/composition/composition.h>\n\n#include <iostream>\n#include <vector>\n#include <string>\n\nint main() {\n    const std::vector<double> comp = {0.708, 2.94e-5, 0.276, 0.003, 0.0011, 9.62e-3, 1.62e-3, 5.16e-4};\n    const std::vector<std::string> symbols = {\"H-1\", \"He-3\", \"He-4\", \"C-12\", \"N-14\", \"O-16\", \"Ne-20\", \"Mg-24\"};\n\n    fourdst::composition::Composition composition;\n    \n    // true puts this in mass fraction mode. false would mean you must provide number fractions instead.\n    composition.registerSymbol(symbols, true); \n    \n    composition.setMassFraction(symbols, comp);\n    composition.finalize(true);\n\n    gridfire::NetIn netIn;\n    netIn.composition = composition;\n    netIn.temperature = 1e7; // In Kelvin\n    netIn.density = 1e2;     // In g/cm^3\n    netIn.energy = 0.0;      // Initial energy in erg\n\n    netIn.tMax = 3.15e17;    // In seconds\n    netIn.dt0 = 1e12;        // Initial time step in seconds\n\n    gridfire::GraphNetwork network(composition);\n    network.exportToDot(\"Network.dot\");\n    gridfire::NetOut netOut;\n    netOut = network.evaluate(netIn);\n    std::cout << netOut << std::endl;    \n}\n\n```\nSave that file to `main.cpp` and compile it with the following command:\n```bash\nclang++ main.cpp $(pkg-config --cflags --libs gridfire) -I/opt/homebrew/include -o main -std=c++23\n./main\n```\n> Note that here I have included the `-I/opt/homebrew/include` flag to specify the header location for boost\n> on my system. This very well may already be in your compiler's include path, or if not it might be in a different\n> location. That is all very system dependent. You can try to get a sense of where the boost headers are located\n> by looking at the `build/compile-commmands.json` file generate by meson after running `meson setup build`.\n\n \n> -std=c++23 or -std=c++20 is required as we use some C++20 feature in GridFire (specifically concepts). Compiling with\n> C++23 is **strongly** recommended.\n\nUsing a different network is as simple as changing the type of the `network` variable to the desired network type. For\nexample, if you wanted to use the `gridfire::Approx8Network`, you would change the line\n`gridfire::GraphNetwork network(composition);` to `gridfire::Approx8Network network(composition);`. The rest of the code\nwould remain the same.\n\nFor more details on how to use the composition library please refer to the libcomposition documentation.\n\n## Funding\nGridFire is a part of the 4D-STAR collaboration.\n\n4D-STAR is funded by European Research Council (ERC) under the Horizon Europe programme (Synergy Grant agreement No.\n101071505: 4D-STAR)\nWork for this project is funded by the European Union. Views and opinions expressed are however those of the author(s)\nonly and do not necessarily reflect those of the European Union or the European Research.\n"
                    }
                }
            },
            "PubPolishPy": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "<h1 align=\"center\"> PubPolishPy </h1>\n<p align=\"center\">\n  <img src=\"https://github.com/tboudreaux/PubPolishPy/blob/master/assets/logo/PubPolishLogo-Dark.png?raw=true\" alt=\"Logo of Project\">\n</p>\n\n# A Python framework for reorganizing tex documents for submission to different sources\nDifferent destinations for TeX documents often require (implicitly or explicitly) different\nstructures of a project. For example, I tend to organize all of my projects in the following structure\n\n\n```bash\n\u251c\u2500\u2500 makefile\n\u251c\u2500\u2500 SelfConsistentModelingOfNGC2808.aux\n\u251c\u2500\u2500 SelfConsistentModelingOfNGC2808.bbl\n\u251c\u2500\u2500 SelfConsistentModelingOfNGC2808.blg\n\u251c\u2500\u2500 SelfConsistentModelingOfNGC2808.log\n\u251c\u2500\u2500 SelfConsistentModelingOfNGC2808.out\n\u251c\u2500\u2500 SelfConsistentModelingOfNGC2808.pdf\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 aasjournal.bst\n\u2502   \u251c\u2500\u2500 aassymbols.tex\n\u2502   \u251c\u2500\u2500 aastex631.cls\n\u2502   \u251c\u2500\u2500 appendicies\n\u2502   \u251c\u2500\u2500 bib\n\u2502   \u2502   \u2514\u2500\u2500 ms.bib\n\u2502   \u251c\u2500\u2500 figures\n\u2502   \u2502   \u251c\u2500\u2500 BestFit.png\n\u2502   \u2502   \u251c\u2500\u2500 BestFitResults.pdf\n\u2502   \u2502   \u251c\u2500\u2500 ClusterAnalysis.pdf\n\u2502   \u2502   \u251c\u2500\u2500 DistributionOfErrors.pdf\n\u2502   \u2502   \u251c\u2500\u2500 ExtractedIsoFit.pdf\n\u2502   \u2502   \u251c\u2500\u2500 HeliumMeanOffset.pdf\n\u2502   \u2502   \u251c\u2500\u2500 notebookFigures -> ../../FigureMakeing/Figures\n\u2502   \u2502   \u2514\u2500\u2500 photometricOffset.pdf\n\u2502   \u251c\u2500\u2500 ms.tex\n\u2502   \u251c\u2500\u2500 natbib.tex\n\u2502   \u251c\u2500\u2500 natnotes.tex\n\u2502   \u2514\u2500\u2500 sections\n\u2502       \u251c\u2500\u2500 AtmopshericModels.tex\n\u2502       \u251c\u2500\u2500 conclusion.tex\n\u2502       \u251c\u2500\u2500 fidanka.tex\n\u2502       \u251c\u2500\u2500 fitting.tex\n\u2502       \u251c\u2500\u2500 intro.tex\n\u2502       \u251c\u2500\u2500 modeling.tex\n\u2502       \u251c\u2500\u2500 observations.tex\n\u2502       \u2514\u2500\u2500 results.tex\n\n```\nThis structure is great for organization while writing the manuscript as it keeps sections separated. However, The Astrophysical Journal implicitly requires that all uploaded manuscripts be in a flat directory structure.\n\nWhile this is not technically challenging to manually update paths it is tedious. It is even more tedious when you have to do it multiple times during the peer review and copy editing process.\n\nPubPolishPy aims to resolve this challenge. \n\nCurrently, PubPolishPy recursively parses your LaTex souce and builds a graph of all of the local file dependencies it can find. These can be \"flattened\" into a directory structure. If you flatten them all the needed files will be copied from where there are to a new folder and all of the paths within the tex document will be updated to point there. Note, this is not an in place operation. <b>NO CHANGES WILL BE MADE TO YOUR ORIGINAL TEX SOURCE</b>. Rather, effectively completely new tex project is built from your existing project but with a different structure.\n\nMore Generally,PubPolishPy aims to provide a framework which destinations can be added into and can parse any tex project to any destination (such as MNRAS, ArXiV, ApJ, or other journals). This is done through the parsers module which implements a generic parser which other parsers can inherit from. Currently only ApJ and ArXiv are implemented.\n\n## Installation\nPubPolishPy may be installed from source. I hope to have it in PyPy soon\n\n```bash\ngit clone git@github.com:tboudreaux/PubPolishPy.git\ncd PubPolishPy\npip instal .\n```\n\nIf you wish to develop for PubPolishPy replace the final line with\n\n```bash\npip install -e .\n```\n\n## Usage\nPubPolishPy may be used through either a command line script or a function call.\n\nUsing pragmatically in python (based on the above example directory structure and assuming the user is in the root (where the makefile it))\n\n```python\nfrom PubPolishPy.parsers import TeXApJFormatter\n\nApJ = TeXApJFormatter(\"src/ms.tex\")\nApJ.migrate()\n```\n\nIf you want to invoke it from the command line use the following script\n\n```bash\npubPolish --target ApJ --dest ApJSubmission src/ms.tex\n```\nValid submission locations are defined in a dictionary which connects the key (ApJ in this case) to the class. The dest folder defines where the flattened project will end up. The script then runs effectively the same code as is presented above\n\n## Targets\nCurrently there are only two targets implemented as these are the targets which I use. If and when I submit to other targets I will build them in, otherwise I am very open to others submitting targets; however, I likely will not work on them spontaneously.\n| Target                       | Command Line Key | Formatter Object Name | Details                                                                                                                                                     |\n| ---------------------------- | ---------------- | --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| The Astrophysical Journal    | ApJ              | TeXApJFormatter       | Flattens the latex directory and copies additional AASTeX files                                                                                             |\n| The ArXiv                    | ArXiv            | TeXArXivFormatter     | Flattens the latex directory, copies additional AASTeX files if available, changes the acknowledgments environment to the macro, downloads the aastex62.cls file if aastex631 is detected, sets the document class options to twocolumn (removes other options if present). |\n| Generic    | Generic              | TeXGenericFormatter       | Does not apply any additional logic.                                                                                             |\n\n\n## Example Makefile\nFor sake of completion I have included a makefile which I use and includes PubPolishPy. Note the make rules for the arxiv and ApJ\n\n```make\nLTC=\"pdflatex\"\nBTC=\"bibtex\"\n\nANAME=\"SelfConsistentModelingOfNGC2808\"\n\nSRCDIR=\"src/\"\nFIGDIR=\"src/figures\"\nBIBDIR=\"src/bibliography\"\nAPEDIR=\"src/appendicies\"\n\nApJDIR=\"ApJ\"\nArXivDIR=\"ArXiv\"\n\nMANFILE=\"ms.tex\"\nMANPATH=\"$(SRCDIR)/$(MANFILE)\"\nApJMANPATH=\"$(ApJDIR)/$(MANFILE)\"\nArXivMANPATH=\"$(ArXivDIR)/$(MANFILE)\"\n\n\nASSET_FILES = $(shell find ./src/ -regex '.*\\(tex\\|pdf\\)$')\n\n\nTFLAGS=\"-jobname=$(ANAME)\"\n\n.PHONY: manuscript apj arxiv clean veryclean\n\ndefault: manuscript\n\nall: manuscrip apj arxiv\n\nmanuscript: ./src/$(ASSET_FILES)\n        $(LTC) $(TFLAGS) $(MANPATH)\n        $(BTC) $(ANAME)\n        $(LTC) $(TFLAGS) $(MANPATH)\n        $(LTC) $(TFLAGS) $(MANPATH)\n\n\napj:\n        if [ -d \"ApJ\" ]; then rm -rf \"ApJ\"; fi\n        pubPolish --target ApJ --dest $(ApJDIR) $(MANPATH)\n        cd $(ApJDIR) && $(LTC) $(TFLAGS) $(MANFILE)\n        cd $(ApJDIR) && $(BTC) $(ANAME)\n        cd $(ApJDIR) && $(LTC) $(TFLAGS) $(MANFILE)\n        cd $(ApJDIR) && $(LTC) $(TFLAGS) $(MANFILE)\n\narxiv:\n        if [ -d \"ArXiv\" ]; then rm -rf \"ArXiv\"; fi\n        pubPolish --target ArXiv --dest $(ArXivDIR) $(MANPATH)\n        cd $(ArXivDIR) && $(LTC) $(TFLAGS) $(MANFILE)\n        cd $(ArXivDIR) && $(BTC) $(ANAME)\n        cd $(ArXivDIR) && $(LTC) $(TFLAGS) $(MANFILE)\n        cd $(ArXivDIR) && $(LTC) $(TFLAGS) $(MANFILE)\n\n\nclean:\n        -rm $(ANAME).blg\n        -rm $(ANAME).bbl\n        -rm $(ANAME).aux\n        -rm $(ANAME).log\n        -rm $(ANAME).out\n\nveryclean: clean\n        -rm $(ANAME).pdf\n```\n\n## Plugins\nRecognizing that every project is different and that it is infeasible for me to impliment a catch all system, PubPolishPy comes with a simple plugin system to allow users to define their own migration logic without diving into fully adding new target classes. \n\n### Building a Plugin\nLet us say that for your project, you want to flatten the directory structure and then after you want to replace all instances of the word foo with the word bar in every tex file. Below I will write a plugin that does this\n\n```python\nfrom PubPolishPy.parsers import TeXGenericFormatter\nfrom PubPolishPy.plugins import PubPolishPlugin\nimport re\n\nCustomPlugin(PubPolishPlugin):\n    def post_migrate(self):\n        for nodeName, nodeData in self.formatter.projectGraph.nodes(data=True):\n            if nodeData.get('tex', False) == True:\n                with open(self.formatter.updatedFilePaths[nodeName], 'r') as f:\n                    content = f.read()\n                newContent = re.sub('foo', 'bar', content)\n                with open(self.formatter.updatedFilePaths[nodeName], 'w') as f:\n                    f.write(newContent)\n\n    def pre_migration(self):\n        self.formatter.flatten() # If using the generic formatter then there is no default mogration logic between pre and post.\n        pass\n\nformatter = TeXGenericFormatter(\"src/main.tex\")\nformatter.register_plugin(CustomPlugin)\nformatter.migrate()\n```\n\nWithin formatter (or any child classes) pre_migrate methods will be called before the migration_logic method which all child classes must impliment while post_migrate methods will be called after migration_logic. ApJ and ArXiv targets impliment migration logic; however, the generic formatter simply passes when migration logic is called allowing any migration logic at all to be written using a plugin. Note that since there is nothing happening between pre and post migration for the generic formatter their names become somewhat confusing. The Key point is that pre always runs first then migration_logic() then post.\n\n"
                    }
                }
            },
            "mplEasyAnimate": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "# What's New?\n - Added the ability to transition between added frames with either a cross dissolve (built in) or using a custom transition function\n - Added Ability to Change the Facecolor of the saved figure\n - Upaded the image loading away from scipy.misc to skimage\n\n# Super simple (and likely stupid) way of making animations with maptlotlib\n\nI wanted to make a plot, then I wanted that plot to move into another plot. That was hard to do, then I foung imageio. But that was slow cause I had to save every figure to disk, then I rememberd that memory existed, then it was faster. Now I have packeged this so it is super easy, now we are here.\n\n## Installation\nYou can either clone the repository and install it or install it via pip. Installing from the repository will get you the latest, possiblly broken, version. The pypi version is much more likley to be working, so if possible I recommend installing with pip.\n\n```bash\ngit clone https://github.com/tboudreaux/mpl_animate.git\ncd mpl_animate\npython setup.py install\n```\nor \n```bash\npip install mplEasyAnimate\n```\n\nalso you will need to install ffmpeg, find instructions for you OS of choice\n\n## API Documentation\nmplEasyAnimate has a relativley simple API, only making use of one (animation) class.\nThe API is however well documented in the code as well as <a href=\"https://algebrist.com/~tboudreaux/docs/mplEasyAnimate/index.html#\">here</a>\n\n## Adding Frames all at once (not recommended)\n\nmplEasyAnimate allow you to build up a list of matplotlib figures and turn them into an animation. You can send a full list in, this works well for small numbers of figures.\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mplEasyAnimate import animation\n\nfigList = list()\nfilename = 'TestAnimation.mp4'\nt = 10\nN = 50\n\nguass = np.random.normal(size=(t, 2, N))\n\n\nfor coord in guass:\n    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n    ax.plot(coord[0], coord[1], 'o')\n    figList.append(fig)\n\nanim = animation(filename)\nanim.add_frames(figList)\nanim.close()\n```\n\nif you want to run this in jupyter the above example should be modified as follows\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mplEasyAnimate import animation\n\nfigList = list()\nfilename = 'TestAnimation.mp4'\nt = 10\nN = 50\n\nguass = np.random.normal(size=(t, 2, N))\n\npit.ioff()\nfor coord in guass:\n    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n    ax.plot(coord[0], coord[1], 'o')\n    figList.append(fig)\n    \nanim = animation(filename, fps=30)\nanim.add_frames(figList)\nanim.close()\nplt.ion()\n```\n\nBasically this keeps jupyter from rendering every figure you make. Note that this is a bad way of making animations if you have more than say 20 frames as it will force your computer to store each of those figures. For animations with thousands of frames than can require more memory than can be allocated and cause your program to crash on an OSError.\n\n\n## Dynamically Adding Frames (sorta recommended)\nHowever, matplotlib will get uphappy if you have too many figures open at once. I reccomend that for more than say 10 figures you dynamically add them (the figures) to the animation instead of doing it all at the end. See below\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mplEasyAnimate import animation\n\n\nfilename = 'TestAnimation.mp4'\nt = 100\nN = 50\n\nguass = np.random.normal(size=(t, 2, N))\n\nanim = animation(filename, fps=60)\nfor i, coord in enumerate(guass):\n    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n    ax.plot(coord[0], coord[1], 'o')\n    anim.add_frame(fig)\n    plt.close(fig)\n\nanim.close()\n```\n\nhere every 10 frames I add all the frames to the animation and then I flush the buffer at the end to make sure that the animation is readable.\n\n\n## Context Manager (very recommended)\nmplEasyAnimate can easily called using context managers which will take care of closing the files for you, here is an example\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mplEasyAnimate import animation\n\n\nfilename = 'TestAnimation.mp4'\nt = 100\nN = 50\n\nguass = np.random.normal(size=(t, 2, N))\n\nwith animation(filename, fps=60) as anim:\n\tfor i, coord in enumerate(guass):\n\t\tfig, ax = plt.subplots(1, 1, figsize=(10, 7))\n\t\tax.plot(coord[0], coord[1], 'o')\n\t\tanim.add_frame(fig)\n\t\tplt.close(fig)\n\n```\n## Smoothing\nmplEasyAnimate can automatically apply a cross dissolve between frames. This is turned on with the autoSmoothing parameter to animation. There is also a smoothingFrames parameter which describes how many frames will be used to dissolve. Note that this smoothes the entire frame, not just the graphed data.\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom mplEasyAnimate import animation\n\n\nfilename = 'TestAnimation.mp4'\nt = 100\nN = 50\n\nguass = np.random.normal(size=(t, 2, N))\n\nwith animation(filename, fps=60, autoSmooth=True, smoothingFrame=60) as anim:\n\tfor i, coord in enumerate(guass):\n\t\tfig, ax = plt.subplots(1, 1, figsize=(10, 7))\n\t\tax.plot(coord[0], coord[1], 'o')\n\t\tanim.add_frame(fig)\n\t\tplt.close(fig)\n\n```\n\n## Saveing the Final Frame\nSometimes when you are giving a talk you want to transition to a slide where the image looks the same as the final frame of the animation; however, because mplEasyAnimate rescales some stuff internally just saving the figure with matplotlib won't provide a seemless transition. mplEasyAnimate allows you to save the final frame of an animation to a png which you can use to make these transitions smooth!\n\n```python\nwith animation(filename, fps=60, saveFinalFrame=True) as anim:\n\tfor i, coord in enumerate(guass):\n\t\tfig, ax = plt.subplots(1, 1, figsize=(10, 7))\n\t\tax.plot(coord[0], coord[1], 'o')\n\t\tanim.add_frame(fig)\n\t\tplt.close(fig)\n\n```\n\nThis will produce a file called \"finalFrame.png\" in your current working directory\n\n## Some Examples\nHere are some examples of output<br>\n![Alt Text](https://github.com/tboudreaux/mpl_animate/blob/master/examples/example.gif?raw=true)\n\nAnother example can be seen here, this shows three views of a the evolution of a Globular Cluster through a couple Mega Years <br>\n![Alt Text](https://github.com/tboudreaux/mpl_animate/blob/master/examples/ClusterAnimation.gif?raw=true)\n\nAn Example showing the developement of exoplanet statistics between the 90s and the 2020s (Credit: <a href=\"https://www.kerockcliffe.com/\">Keighley Rockcliffe</a>)\n\n\nhttps://user-images.githubusercontent.com/6620251/225698910-994ea8a3-496c-4331-9ddb-4acff3405f1f.mp4\n\n\n## Speed\nThere are some ways to get speed / preformance improvments when making an animation\n\n 1) Turn down the dpi, when you make an animation objects you can set the dpi as a keyword argument, lower dpis will render faster\n 2) Reuse the same figure, consider the following code and how it reuses the same figure instead of creating a new one for each frame\n\n```python\nnumber = 100\nrlayers = 100\n\ncirc, equib = make_circ(1, 1, number)\nanim = animation('Animations/DrawStar_{}x{}.mp4'.format(number, rlayers), fps=30, dpi=5)\nfig, ax = plt.subplots(1, 1, figsize=(10, 7))\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\nax.axis('off')\n\nfor particle in tqdm(range(1, number)):\n    ax.plot(circ[particle-1:particle+1, 0, 0, 0], circ[particle-1:particle+1, 0, 0, 1], 'C0o-', linewidth=1)\n\n    anim.add_frame(fig)\n\n\nd0 = np.pi/(rlayers/2)\nfor theta in tqdm(np.arange(d0, 2*np.pi-d0/2, d0)):\n    R = np.sqrt((circ[:, 0, 0, 0]**2) + (circ[:, 0, 0, 1]**2))\n    ax.plot(R*np.cos(theta), R*np.sin(theta), 'C0o-', linewidth=1)\n    \n    anim.add_frame(fig)\n    \nfor theta in tqdm(np.arange(d0, 2*np.pi+d0, d0)):\n    R = np.sqrt((circ[:, 0, 0, 0]**2) + (circ[:, 0, 0, 1]**2))\n    \n    x1, y1 = R*np.cos(theta), R*np.sin(theta)\n    x2, y2 = R*np.cos(theta+d0), R*np.sin(theta+d0)\n    ax.plot([x1, x2], [y1, y2], 'C0', linewidth=1)\n    anim.add_frame(fig)\n\nplt.close(fig)\nanim.close()\n```\n 3) turn off the axes, drawing the x and y axes is one of the slowest parts of matplotlib's drawing process, if they are not nessicairy for the animation consider turning them off (see the above code block).\n\n\n"
                    }
                }
            },
            "opat-core": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "<p align=\"center\">\n  <img src=\"assets/logo/opatCoreLogo.png\" width=\"300\" alt=\"OPAT Core Libraries Logo\">\n</p>\n\n<h1 align=\"center\">OPAT Core Libraries</h1>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/opatio/\"><img src=\"https://img.shields.io/pypi/v/opatio\" alt=\"PyPI - Version\"></a>\n  <a href=\"https://github.com/4D-STAR/opat-core/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/4D-STAR/opat-core\" alt=\"GitHub License\"></a>\n  <a href=\"https://4d-star.github.io/opat-core/\"><img src=\"https://img.shields.io/badge/Documentation-View%20Here-blue\" alt=\"Documentation\"></a>\n</p>\n\n---\n\nThis repository contains the core C++ library, python module, and file specification for the OPAT file format. \n\nOPAT is a structured binary file format developed by the 4D-STAR collaboration for efficient and standardized storage of set of tabular data indexed by some floating point vector (such as composition).\n\nThe general principle behind OPAT is that an arbitrary number of data cards are stored, indexed by some arbitrary-length composition vector. Each data card contains an arbitrary number of data tables indexed by some string tag.\n\nThe provided python module can be used to create and read OPAT files while the C++ module is intended to be used to interface OPAT tables with C++ code for reading (but not currently generation).\n\n## Installation\nThis repository provides both C++ and python bindings. The first thing to note is that these do not depend on each other at all. If you want to generate OPAT tables and/or use them in python code you will want the python module. If you want to use OPAT tables in C++ code you will want the C++ module. \n\nThere are more details on usage for each language in their respective folder; however, broad installation instructions are included here as well.\n\n### Dependencies Overview\n\nBelow is a summary of the key dependencies for each part of the project.\n\n#### Python Dependencies\nThese are managed by `pip` and are listed in `opatIO-py/pyproject.toml`.\n\n| Dependency  | Minimum Version | Usage                                  | Source/Authors                                                                 |\n|-------------|-----------------|----------------------------------------|--------------------------------------------------------------------------------|\n| `numpy`     | `>= 1.21.1`     | Numerical operations, array handling   | [NumPy Developers](https://numpy.org/)                                         |\n| `xxhash`    | `>= 3.5.0`      | Fast hashing algorithms (internal use) | [Yann Collet (Cyan4973)](https://github.com/Cyan4973/xxHash)                   |\n| `scipy`     | `>= 1.15.0`     | Triangulation (for interpolation)      | [SciPy Developers](https://scipy.org/)                                         |\n\n#### C++ Dependencies\nMost C++ dependencies are handled via Meson's wrap system (built at compile time). Boost is an exception.\n\n| Dependency | Installation                                     | Usage                                                                 | Source/Authors                                                                                                                                                                     |\n|------------|--------------------------------------------------|-----------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Boost      | System install or via bundled script (prompts user) | Numerical linear algebra (`ublas`) in `TableLattice`                  | [Boost Community](https://www.boost.org/)                                                                                                                                          |\n| Qhull      | Meson wrap (built automatically)                 | N-dimensional Delaunay triangulation for `TableLattice`               | [Barber, C.B., Dobkin, D.P., and Huhdanpaa, H.T., \"The Quickhull algorithm for convex hulls,\" ACM Trans. on Mathematical Software, 22(4):469-483, Dec 1996](http://www.qhull.org/) |\n| xxHash     | Meson wrap (built automatically)                 | Fast hashing for `FloatIndexVector` lookups in `OPAT` class           | [Yann Collet (Cyan4973)](https://github.com/Cyan4973/xxHash) & [Stefan Brumme (stbrumme)](https://create.stephan-brumme.com/xxhash/)                                                                     \n| PicoSHA2   | Meson wrap (built automatically)                 | SHA-256 hashing for data integrity checks (`CardCatalogEntry`)        | [Shintarou Okada (okdshin)](https://github.com/okdshin/PicoSHA2)                                                                                                                             \n| cxxopts    | Meson wrap (built automatically)                 | Command-line option parsing for CLI tools (e.g., `opatHeader`)        | [Jarryd Beck (jarro2783)](https://github.com/jarro2783/cxxopts)                                                                                                                                \n\n### Python Installation\n```bash\npip install opatio\n```\n\n### C++ Installation\nYou will need `meson`, `cmake`, and `ninja` installed pre-installed. Note that cmake is needed in order to build subprojects which use CMake as their build system. *opat-core does not make use of CMake as a build system*. These can be installed with pip\n```bash\npip install \"meson>=1.6.0\"\npip install cmake\npip install ninja\n```\nThen you can build and install opat-core\n```bash\ngit clone https://github.com/4D-STAR/opat-core\ncd opat-core\nmeson setup build --buildtype=release\nmeson compile -C build\n```\nIf you want to run tests you can use meson's build in test command. Note that due to small numerical differences between computers and compilers, some tests may fail. This is expected and can be ignored. Specifically, `TableLattice` tests rely on the ordering / adjacency of Delaunay triangulation which can vary between systems.\n```bash\nmeson test -C build\n```\n\n\nTo install headers, libraries, the pkg_config file (`opatIO.pc`), and the command line utilities\n```bash\nmeson install -C build\n```\n\n---\n\n# Usage\n\n## Python Usage\nSee the `opatIO-py` directory for detailed installation and usage instructions. The Python module allows for both creating and reading OPAT files. A comprehensive API manual can also be found in `opatIO-py/docs/build/latex/opatio.pdf`.\n\nThe general workflow involves creating an `OPAT` object, adding tables to it, and then saving it.\n\n### Creating and Writing an OPAT File\n```python\nfrom opatio import OPAT\n\n# Initialize an OPAT object\nopacityFile = OPAT()\nopacityFile.set_comment(\"This is a sample opacity file\")\nopacityFile.set_source(\"OPLIB\") # Optional: set a source identifier\n\n# Assume you have your data:\n# X, Z: components of the index vector (e.g., Hydrogen and Metal mass fractions)\n# logR: 1D array of log(R) values (e.g., density variable)\n# logT: 1D array of log(T) values (e.g., temperature variable)\n# logKappa: 2D array of log(opacity) values, with shape (len(logR), len(logT))\nX, Z = 0.7, 0.02\nlogR = [1.0, 2.0, 3.0] \nlogT = [4.0, 5.0, 6.0, 7.0]\nlogKappa = [[1.1, 1.2, 1.3, 1.4], [2.1, 2.2, 2.3, 2.4], [3.1, 3.2, 3.3, 3.4]]\n\n\n# Add a table to a new DataCard, indexed by (X, Z)\n# The tag \"data\" identifies this specific table within the DataCard\ncard = opacityFile.add_table(index_vector=(X, Z), \n                             tag=\"data\", \n                             row_values=logR, \n                             col_values=logT, \n                             table_data=logKappa, \n                             rowName=\"logR\", \n                             columnName=\"logT\")\n\n# You can add another table to the same DataCard\n# For example, interpolation coefficients\n# xOrder, yOrder, coeff = ... (your coefficient data)\n# opacityFile.add_table(index_vector=(X,Z), tag=\"interp_coeffs\", \n#                       row_values=x_coeffs_indices, col_values=y_coeffs_indices, table_data=coeffs,\n#                       card=card) # Pass the existing card object\n\n# Save the OPAT file\nopacityFile.save(\"opacity.opat\")\n\n# Optionally, save an ASCII representation (for debugging or human-readable inspection)\nopacityFile.save_as_ascii(\"opacity_ascii.txt\")\n\nprint(\"OPAT file created and saved.\")\n```\n\n### Reading an OPAT File\n```python\nfrom opatio import read_opat\n\n# Load an existing OPAT file\nopacityFile = read_opat(\"opacity.opat\")\n\n# Print the header information\nprint(opacityFile.header)\n\n# Access a specific DataCard by its index vector\n# Note: Index vectors are tuples. For floating point comparisons, \n# direct dictionary-like access might require exact matches.\n# It's often better to iterate or use methods that handle floating point precision if needed.\n# For this example, we assume the index (0.7, 0.02) exists.\ntarget_index = (0.7, 0.02)\nif target_index in opacityFile.cards:\n    data_card = opacityFile.cards[target_index]\n    \n    # Access a specific table within the DataCard by its tag\n    if \"data\" in data_card.tables:\n        table = data_card.tables[\"data\"]\n        print(f\"\\nTable 'data' for index {target_index}:\")\n        print(f\"Row Names (logR): {table.row_values}\")\n        print(f\"Column Names (logT): {table.col_values}\")\n        print(f\"Table Data (logKappa):\\n{table.table_data}\")\n    else:\n        print(f\"Table 'data' not found in card with index {target_index}\")\nelse:\n    print(f\"DataCard with index {target_index} not found.\")\n\n# Iterate through all cards and their tables\nfor index_vec, card in opacityFile.cards.items():\n    print(f\"\\nDataCard at index: {index_vec}\")\n    for tag, table in card.tables.items():\n        print(f\"  Table tag: {tag}, Shape: ({table.num_rows}, {table.num_cols})\")\n```\n\n### Converting OPAL Type I to OPAT\nA utility is provided to convert common OPAL Type I opacity files to the OPAT format.\n```python\nfrom opatio.convert import OPALI_2_OPAT\n\n# Assuming an OPAL Type I file named \"GS98hz\" is in the current directory\nOPALI_2_OPAT(\"GS98hz\", \"gs98hz.opat\")\nprint(\"Converted OPAL Type I file to gs98hz.opat\")\n```\n\n## C++ Library/Header Usage\nThe C++ library is primarily designed for reading and interpolating data from OPAT files. Below are some common usage examples. For a more detailed API manual, refer to the Doxygen documentation generated in `opatIO-cpp/docs/html/index.html` (or the PDF version in `opatIO-cpp/docs/latex/refman.pdf`).\n\n### Linking the C++ Library\n\nThe installation process detailed above generate a package config file `opatIO.pc` that can be used to link against the\nC++ library. You can use this in your `CMakeLists.txt`, `meson.build` files, or directly in your compiler commands. \n\nas a simple example lets say you make a file called `main.cpp` with the following contents:\n```c++\n#include <opatIO.h>\n\nint main() {\n    opat::OPAT opat_file = opat::readOPAT(\"example.opat\");\n    std::cout << \"Successfully read OPAT file: example.opat\" << std::endl;\n}\n```\nif you were to just run\n```bash\ng++ main.cpp -o main\n```\nThis would fail because the compiler neither knows where to find the `opatIO.h` header file nor the `libopatio.a` library.\n\nHowever, if you use the `pkg-config` tool, you can easily compile and link against the OPAT library:\n```bash\ng++ main.cpp -o main $(pkg-config --cflags --libs opatIO) --std=c++23\n```\n\n> \u26a0\ufe0f The `--std=c++23` flag is required as the OPAT C++ library makes use of C++23 features. --std=c++17 is the minimum required version, but C++23 is recommended for full compatibility.\n> \n\n### C++ API Usage Examples\n\n#### Accessing a Table by Index and Tag\n\n```c++\n#include \"opatIO.h\"\n#include \"indexVector.h\" // For FloatIndexVector\n#include <string>\n#include <vector>\n#include <iostream>\n\nint main() {\n    std::string filename = \"example.opat\";\n    try {\n        opat::OPAT opat_file = opat::readOPAT(filename);\n        // Define the index vector for the DataCard you want to access\n        FloatIndexVector target_index({0.35, 0.004}); // Example index\n        // Define the tag for the table within the DataCard\n        std::string table_tag = \"data\"; // Example tag\n\n        const opat::DataCard& data_card = opat_file[target_index];\n        const opat::OPATTable& table = data_card[table_tag];\n        \n        std::cout << \"Table '\" << table_tag << \"' at index \" << target_index << \":\" << std::endl;\n        table.print(); // Print the table data\n    } catch (const std::exception& e) {\n        std::cerr << \"Error accessing table: \" << e.what() << std::endl;\n        return 1;\n    }\n    return 0;\n}\n```\n\n#### Slicing a Table\n```c++\n#include \"opatIO.h\"\n#include \"indexVector.h\"\n#include <string>\n#include <vector>\n#include <iostream>\n\nint main() {\n    std::string filename = \"example.opat\";\n    try {\n        opat::OPAT opat_file = opat::readOPAT(filename);\n        FloatIndexVector target_index({0.35, 0.004});\n        std::string table_tag = \"data\";\n\n        // Define slices for rows and columns\n        opat::Slice row_slice(0, 6);    // Rows 0 through 5\n        opat::Slice col_slice(25, 36);  // Columns 25 through 35\n\n        const opat::OPATTable& original_table = opat_file[target_index][table_tag];\n        opat::OPATTable sliced_table = original_table.slice(row_slice, col_slice);\n        \n        std::cout << \"Sliced table '\" << table_tag << \"' at index \" << target_index << \":\" << std::endl;\n        sliced_table.print(); // Print the sliced table\n    } catch (const std::exception& e) {\n        std::cerr << \"Error slicing table: \" << e.what() << std::endl;\n        return 1;\n    }\n    return 0;\n}\n```\n\n#### Using `TableLattice` for Interpolation\nThe `TableLattice` class allows for N-dimensional linear interpolation of data within an OPAT file.\n\n```c++\n#include \"opatIO.h\"\n#include \"indexVector.h\"\n#include \"tableLattice.h\" // For TableLattice\n#include <string>\n#include <vector>\n#include <iostream>\n\nint main() {\n    std::string opat_filename = \"your_data.opat\"; // Replace with your OPAT file\n    try {\n        opat::OPAT opat_data = opat::readOPAT(opat_filename);\n\n        // Initialize TableLattice with the loaded OPAT data\n        opat::lattice::TableLattice lattice(opat_data);\n        std::cout << \"TableLattice initialized.\" << std::endl;\n\n        // Define the index vector for which you want to interpolate data\n        // Ensure its dimension matches opat_data.header.numIndex\n        FloatIndexVector query_point({0.54, 0.07}); // Example for a 2D OPAT\n\n        // Get the interpolated DataCard\n        opat::DataCard interpolated_card = lattice.get(query_point);\n        std::cout << \"Interpolated DataCard retrieved for \" << query_point << std::endl;\n\n        // Access tables from the interpolated_card as needed\n        // For example, if your OPAT files contain a table tagged \"density\":\n        // if (!interpolated_card.getKeys().empty()) {\n        //     const opat::OPATTable& density_table = interpolated_card[interpolated_card.getKeys()[0]];\n        //     density_table.print();\n        // }\n\n    } catch (const std::exception& e) {\n        std::cerr << \"An error occurred: \" << e.what() << std::endl;\n        return 1;\n    }\n    return 0;\n}\n```\n\n## Command Line Utility Usage\nAfter running `meson install -C build`, three command-line utilities will be available in your system's path (you might need to resource your shell, e.g., `source ~/.bashrc` or `source ~/.zshrc`, or open a new terminal). These utilities are:\n\n1.  **`opatHeader`**: Prints out the main header information of an OPAT file.\n    ```bash\n    opatHeader --file path/to/your/file.opat\n    ```\n\n2.  **`opatInspect`**: Prints the main header and the card catalog (index of all data cards) of an OPAT file.\n    ```bash\n    opatInspect --file path/to/your/file.opat\n    ```\n\n3.  **`opatVerify`**: Verifies if the provided file is a valid OPAT file according to the specification. It checks the magic number and performs other integrity checks.\n    ```bash\n    opatVerify --file path/to/your/file.opat\n    ```\n\nNote the `--file` (or `-f`) flag is required before providing the path to the OPAT file for all tools.\n\n---\n\n## Fortran Usage\nA Fortran wrapper around the C++ `opatIO` module is provided. This interface allows Fortran applications to read data from OPAT files. Note that this is primarily for reading and may not receive the same level of feature development as the C++ and Python interfaces.\n\nThe Fortran module and object files are typically built into `build/opatIO-fortran/libopatio_f.a` (and associated `.mod` files in `build/opatIO-fortran/libopatio_f.a.p`). You will need to link against this library and potentially the C++ `opatIO` library as well.\n\nRefer to `opatIO-fortran/readme.md` and `opatIO-cpp/docs/static/fortran.md` for more details.\n\n### Example: Reading an OPAT File in Fortran\n\nThe core workflow involves:\n1. Loading the OPAT file using `load_opat_file`.\n2. Retrieving a specific table using `get_opat_table`.\n3. Checking the returned table structure for errors.\n4. Accessing table data.\n5. Freeing resources with `free_opat_file`.\n\n```fortran\nprogram opat_fortran_example\n    use opat_interface_module\n    use, intrinsic :: iso_c_binding\n    implicit none\n\n    character(len=256)      :: filename\n    real(c_double), target  :: index_vec(2) ! Assuming 2D index vector\n    character(len=32)       :: tag          ! Ensure tag length matches definition in module\n    type(opat_table_f_type) :: my_table\n    integer                 :: load_status\n    integer                 :: i, j\n\n    ! --- Configuration ---\n    filename = 'gs98hz.opat'  ! Ensure this file exists where the program is run\n                              ! A copy can be found in opatIO-cpp/examples\n    index_vec(1) = 0.9_c_double   ! Example X value\n    index_vec(2) = 0.08_c_double  ! Example Z value\n    tag = 'data'              ! Tag of the table to retrieve\n\n    ! --- Load OPAT File ---\n    print *, \"Attempting to load OPAT file: \", trim(filename)\n    load_status = load_opat_file(trim(filename))\n    if (load_status /= 0) then\n        print *, \"ERROR: Failed to load OPAT file. Status: \", load_status\n        stop 1\n    end if\n    print *, \"OPAT file loaded successfully.\"\n\n    ! --- Get Table ---\n    print *, \"Attempting to retrieve table with tag '\", trim(tag), \"' for index [\", index_vec(1), \",\", index_vec(2), \"]\"\n    call get_opat_table(index_vec, trim(tag), my_table)\n\n    ! --- Check for Errors and Process Table ---\n    if (my_table%error_code /= 0) then\n        print *, \"ERROR: Failed to retrieve table. Error code: \", my_table%error_code\n        if (my_table%error_code == 1) print *, \" (Likely cause: Index vector not found)\"\n        if (my_table%error_code == 2) print *, \" (Likely cause: Table tag not found for given index)\"\n    else\n        print *, \"Table retrieved successfully!\"\n        print *, \"Table dimensions: \", my_table%num_rows, \"rows, \", my_table%num_cols, \"cols\"\n\n        ! Check if pointers are associated (important!)\n        if (.not. c_associated(my_table%row_values)) then\n            print *, \"ERROR: Row values pointer not associated.\"\n        else if (.not. c_associated(my_table%col_values)) then\n            print *, \"ERROR: Column values pointer not associated.\"\n        else if (.not. c_associated(my_table%data)) then\n            print *, \"ERROR: Table data pointer not associated.\"\n        else\n            ! Access and print some data (example)\n            print *, \"First row value: \", my_table%row_values(1)\n            print *, \"First col value: \", my_table%col_values(1)\n            print *, \"Data at (1,1): \", my_table%data(1,1)\n\n            ! Example: Print a small part of the table\n            ! print *, \"Sample data:\"\n            ! do i = 1, min(my_table%num_rows, 3)\n            !    write(*,'(3(F8.3,X))') (my_table%data(i,j), j = 1, min(my_table%num_cols,3))\n            ! end do\n        end if\n    end if\n\n    ! --- Free OPAT File Resources ---\n    print *, \"Freeing OPAT file resources.\"\n    call free_opat_file()\n    print *, \"Program finished.\"\n\nend program opat_fortran_example\n```\n**Important Notes for Fortran Usage:**\n- Ensure the OPAT file (e.g., `gs98hz.opat`) is accessible in the directory where the Fortran executable is run.\n- Always check `my_table%error_code` after calling `get_opat_table`.\n- Crucially, call `free_opat_file()` when you are done with the OPAT file to prevent memory leaks.\n- The dimensions of `index_vec` must match the `numIndex` defined in the OPAT file's header.\n- The length of the `tag` character variable should be sufficient for the tags used in your OPAT files.\n\n"
                    }
                }
            },
            "readme.txt": {
                "type": "file",
                "content": "cd into a project to see more info on it!\n"
            },
            "splitAxes": {
                "type": "dir",
                "children": {
                    "readme.md": {
                        "type": "file",
                        "content": "![version](https://badgen.net/pypi/v/splitAxes)\n# Simple Split X axes for matplotlib\n\nUse gridspec to generate nxm grids of figures where the x axis on each figure\nmay be an split arbitrary number of times.\n\nThe axes object presented by this package is similar (though not identical) to\nthe axes object presented by matplotlib. This allows for a good amount of plotting\ncode to be directly ported.\n\n## Installation\n\n### From source\n ```bash\n git clone https://github.com/tboudreaux/splitAxes.git\n cd splitAxes\n pip instal -e .\n ```\n### From PyPi\n ```bash\n pip install splitAxes\n ```\n\n## Examples\n```python\nfrom splitAxes import split_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfigRows = 2\nfigColums = 2\n\n# this is a matrix of size rows x colums. Each entry in the matrix is the number of splits (NOT the number of final panels, which will be 1 + the number of splits) to generate\n\nsplitMatrix=np.array([[3,1],[0,2]])\n\nfig, axs = split_grid(figRows, figColums, splitMatrix, figsize=(15,7))\n\n# Example Data\nX = np.linspace(-10,50, 1000)\nY = np.exp(-(X-0)**2) + np.exp(-(X-10)**2) + np.exp(-(X-20)**2)\n\naxs[0,0].set_xlim(0,45)\naxs[0,0].set_ylabel(\"Bob\")\naxs[1,0].plot(X,Y, color='green')\naxs[1,1].scatter([0,3,4],[5,4,2])\naxs[0,1].set_xlabel(\"Dave\", position=\"manual\", labelpos=0.67)\n\naxs[1,0].fill_between([0,10], 0.3, alpha=0.5, color='blue')\n\nplt.show()\n```\n\nThis should produce an output that looks something like\n\n![Example Output](tests/ExampleOutput.png?raw=true \"Example Output\")\n\n"
                    }
                }
            }
        }
    },
    "readme.txt": {
        "type": "file",
        "content": "# Emily M. Boudreaux (they/she)\n\n## About Me\n\nI am a Postdoctoral Research Associate and an Astronomy Lecturer at Dartmouth College. My work is centered on computational astrophysics, with a specific focus on the stellar evolution of low-mass and cool stars. Currently, I am contributing to the ERC Synergy Grant 4D-STAR under the guidance of Aaron Dotter.\n\nMy research involves analyzing the chemical composition of globular clusters and developing advanced numerical tools to improve stellar evolution models.\n\n## Background\n\n- **Ph.D. in Physics & Astronomy** | Dartmouth College (2024)\n  - Advisor: Dr. Brian C. Chaboyer\n  - Thesis: \"Models of Low Mass Stars as Physical Laboratories\"\n\n- **B.S. in Computational Physics** | High Point University (2019)\n\n## Projects & Interests\n\n- **Blog:** I run [astrobugs.com](http://astrobugs.com), a blog dedicated to exploring advanced computer science topics for graduate students in astronomy.\n\n- **Hobbies:** Outside of astrophysics, I enjoy theater tech and carpentry, I used to voluntere at Encore Stage & Studio. I am also an avid Star Trek fan.\n\n## Contact\n\n- **Email:** emily@boudreauxmail.com\n- **GitHub:** https://github.com/tboudreaux\n\n"
    }
}
};
